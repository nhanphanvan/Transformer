{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "En-Vi Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tK3acOHhNWXP",
        "KXyFbRYFNcxG",
        "M-cv5Z4VN42W",
        "lxQa9yrFOAM9",
        "Y9pTY391ONF-",
        "6SINsPiHOcjF",
        "rWu__0yNOrPW",
        "HWoP3tIXOy90",
        "caviQnKdO9mM",
        "rgyii8Fs-muU",
        "3GyJl7LAotpn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhanphanvan/Transformer/blob/main/En_Vi_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yone6pXhM3uQ"
      },
      "source": [
        "# Neural Machine Translation with Transformer and Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3acOHhNWXP"
      },
      "source": [
        "### 1. Install Transformer package and extract data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42am2j_pU2eW"
      },
      "source": [
        "!git clone https://github.com/nhanphanvan/Transformer.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_qiAeFuBdkW"
      },
      "source": [
        "import os, sys, tarfile\n",
        "\n",
        "def extract(tar_url, extract_path='.'):\n",
        "    print(tar_url)\n",
        "    tar = tarfile.open(tar_url, 'r')\n",
        "    for item in tar:\n",
        "        tar.extract(item, extract_path)\n",
        "        if item.name.find(\".tgz\") != -1 or item.name.find(\".tar\") != -1:\n",
        "            extract(item.name, \"./\" + item.name[:item.name.rfind('/')])\n",
        "\n",
        "# try:\n",
        "#     extract(dev_path)\n",
        "#     extract(test_path)\n",
        "#     extract(train_path)\n",
        "#     print('Done.')\n",
        "# except:\n",
        "#     print('Error')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDS1PYMTNPRN"
      },
      "source": [
        "dev_path = './Transformer/data/en-vi-translation/zip-file/dev-2012-en-vi.tgz'\n",
        "test_path = './Transformer/data/en-vi-translation/zip-file/test-2013-en-vi.tgz'\n",
        "train_path = './Transformer/data/en-vi-translation/zip-file/train-en-vi.tgz'\n",
        "\n",
        "train_src_path = './Transformer/data/en-vi-translation/split-data/short_train.en'\n",
        "train_tgt_path = './Transformer/data/en-vi-translation/split-data/short_train.vi'\n",
        "dev_src_path = './Transformer/data/en-vi-translation/split-data/short_dev.en'\n",
        "dev_tgt_path = './Transformer/data/en-vi-translation/split-data/short_dev.vi'\n",
        "test_src_path = './Transformer/data/en-vi-translation/split-data/short_test.en'\n",
        "test_tgt_path = './Transformer/data/en-vi-translation/split-data/short_test.vi'\n",
        "\n",
        "medical_train_src_path = './Transformer/data/en-vi-translation/medical-data/long_medical_set.en'\n",
        "medical_train_tgt_path = './Transformer/data/en-vi-translation/medical-data/long_medical_set.vi'\n",
        "medical_test_src_path = './Transformer/data/en-vi-translation/medical-data/medical_test_set.en'\n",
        "medical_test_tgt_path = './Transformer/data/en-vi-translation/medical-data/medical_test_set.vi'\n",
        "\n",
        "# folder to save model and optimizer during training and after trained\n",
        "FOLDER_PATH = './'\n",
        "RESULT_PATH = FOLDER_PATH + \"result.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXyFbRYFNcxG"
      },
      "source": [
        "### 2. Build Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjdhZjwCnjx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import Tensor\n",
        "from typing import Optional"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2x1jsfnzoPR"
      },
      "source": [
        "def segment_sentence(sentence):\n",
        "  sentences = rdrsegmenter.tokenize(sentence) \n",
        "  sentences = [\" \".join(sentence) for sentence in sentences]\n",
        "  sentence = \" \".join(sentences).strip()\n",
        "  return sentence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-VLZKAPC9zm"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, src_path, tgt_path, segment=None):\n",
        "    with open(src_path, 'r', encoding='utf-8') as file:\n",
        "      src = file.read().splitlines()\n",
        "    with open(tgt_path, 'r', encoding='utf-8') as file:\n",
        "      tgt = file.read().splitlines()\n",
        "    if segment is not None:\n",
        "      if segment:\n",
        "        src = [segment_sentence(sentence) for sentence in src]\n",
        "      else:\n",
        "        tgt = [segment_sentence(sentence) for sentence in tgt]\n",
        "    self.samples = list(zip(src, tgt))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.samples[index]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-cv5Z4VN42W"
      },
      "source": [
        "### 3. Install vncorenlp and transformers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96av2kZjGBVO"
      },
      "source": [
        "!pip -q install transformers\n",
        "!pip -q install vncorenlp\n",
        "!pip -q install fairseq\n",
        "!pip -q install fastBPE\n",
        "\n",
        "!pip -q install fastapi\n",
        "!pip -q install uvicorn\n",
        "!pip -q install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx_eX_DEGPen"
      },
      "source": [
        "!gdown https://drive.google.com/a/gm.uit.edu.vn/uc?id=1pXJZ9eHp6DWkQ5MhCzmWYsKyLQEDiodz&export=download\n",
        "!tar xzf /content/vn_sbert_deploy.tar.gz\n",
        "\n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6xXtjkjK6K4"
      },
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SRC_VOCAB_SIZE = 28996\n",
        "TGT_VOCAB_SIZE = 64001\n",
        "HIDDEN_SIZE = 768\n",
        "NUM_ENCODER_LAYERS = 12\n",
        "NUM_DECODER_LAYERS = 12\n",
        "NUM_ATTENTION_HEADS = 12\n",
        "FEEDFORWARD_SIZE = 3072\n",
        "DROPOUT = 0.1\n",
        "ACTIVATION = 'gelu'\n",
        "LAYER_NORM_EPS = 1e-12\n",
        "SRC_UNK_ID, SRC_PADDING_ID, SRC_BOS_ID, SRC_EOS_ID = 100, 0, 101, 102\n",
        "TGT_UNK_ID, TGT_PADDING_ID, TGT_BOS_ID, TGT_EOS_ID = 3, 1, 0, 2\n",
        "NORM_FIRST = True\n",
        "MAX_SEQUENCE_LENGTH = 1024\n",
        "BATCH_SIZE = 10\n",
        "BERT_EMBEDDING = True\n",
        "OUTPUT_HIDDEN_STATES = True\n",
        "APPLY_LAYER_NORM = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLpNmUGtGQAv"
      },
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig, BertModel, RobertaModel\n",
        "\n",
        "rdrsegmenter = VnCoreNLP(\"./vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "\n",
        "src_model_id = 'bert-base-cased'\n",
        "tgt_model_id = 'vinai/phobert-base'\n",
        "\n",
        "src_config = AutoConfig.from_pretrained(src_model_id)\n",
        "# src_bert = AutoModel.from_pretrained(src_model_id, config=src_config)\n",
        "src_tokenizer = AutoTokenizer.from_pretrained(src_model_id)\n",
        "src_tokenizer.model_max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "tgt_config = AutoConfig.from_pretrained(tgt_model_id)\n",
        "# tgt_bert = AutoModel.from_pretrained(tgt_model_id, config=tgt_config)\n",
        "tgt_tokenizer = AutoTokenizer.from_pretrained(tgt_model_id)\n",
        "tgt_tokenizer.model_max_length = MAX_SEQUENCE_LENGTH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxQa9yrFOAM9"
      },
      "source": [
        "### 4. Init Custom Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFmcd3PrC-U8"
      },
      "source": [
        "from Transformer.modules.config import TransformerConfig\n",
        "from Transformer.modules.transformer import Transformer\n",
        "from Transformer.modules.embedding import PositionalEmbedding, TransformerEmbedding\n",
        "from Transformer.modules.seq2seq_transformer import Seq2SeqTransformer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKYllcWiSkup"
      },
      "source": [
        "kwargs = {\n",
        "    'src_vocab_size': SRC_VOCAB_SIZE,\n",
        "    'tgt_vocab_size': TGT_VOCAB_SIZE,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'num_encoder_layers': NUM_ENCODER_LAYERS,\n",
        "    'num_decoder_layers': NUM_DECODER_LAYERS,\n",
        "    'num_attention_heads': NUM_ATTENTION_HEADS,\n",
        "    'feedforward_size': FEEDFORWARD_SIZE,\n",
        "    'dropout': DROPOUT,\n",
        "    'activation': ACTIVATION,\n",
        "    'layer_norm_eps': LAYER_NORM_EPS,\n",
        "    'src_padding_id': SRC_PADDING_ID,\n",
        "    'tgt_padding_id': TGT_PADDING_ID,\n",
        "    'norm_first': NORM_FIRST,\n",
        "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
        "    'bert_embedding': BERT_EMBEDDING,\n",
        "    'output_hidden_states': OUTPUT_HIDDEN_STATES,\n",
        "    'apply_layer_norm': APPLY_LAYER_NORM,\n",
        "    'device': DEVICE,\n",
        "    'dtype': torch.float32\n",
        "}\n",
        "\n",
        "config = TransformerConfig(**kwargs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1JwMDncaa8i"
      },
      "source": [
        "# Note: This part is incredibly important. \n",
        "# Need to train with this setup of the model is very unstable.\n",
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the warmup scheduler as a :class:`dict`.\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer.\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the warmup scheduler's state.\n",
        "        Arguments:\n",
        "            state_dict (dict): warmup scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict) \n",
        "    \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
        "        \n",
        "def get_std_opt(model, d_model):\n",
        "    return NoamOpt(d_model, 0.25, 8000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAQ_--ILRsJ5"
      },
      "source": [
        "transformer = Seq2SeqTransformer(config=config)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=TGT_PADDING_ID, label_smoothing=0.1)\n",
        "\n",
        "# optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "# optimizer = torch.optim.SGD(transformer.parameters(), lr=0.0001, momentum=0.9, nesterov=True)\n",
        "optimizer = get_std_opt(transformer, HIDDEN_SIZE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9pTY391ONF-"
      },
      "source": [
        "### 5. Utility Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgSZUqkoRsPS"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 0).transpose(0, 1)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[1]\n",
        "    tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == SRC_PADDING_ID)\n",
        "    tgt_padding_mask = (tgt == TGT_PADDING_ID)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysxwnkwh2iUO"
      },
      "source": [
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    # src_batch, tgt_batch = [], []\n",
        "    # for src_sample, tgt_sample in batch:\n",
        "    #     src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "    #     tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    # src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    # tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    # return src_batch, tgt_batch\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(src_sample.rstrip(\"\\n\"))\n",
        "        tgt_batch.append(tgt_sample.rstrip(\"\\n\"))\n",
        "    src_encodings = src_tokenizer.batch_encode_plus(src_batch, padding=True)\n",
        "    src_ids = torch.tensor(src_encodings.get('input_ids'))\n",
        "    # src_attention_masks = torch.tensor(src_encodings.get('attention_mask'))\n",
        "    \n",
        "    tgt_encodings = tgt_tokenizer.batch_encode_plus(tgt_batch, padding=True)\n",
        "    tgt_ids = torch.tensor(tgt_encodings.get('input_ids'))\n",
        "    # tgt_attention_masks = torch.tensor(tgt_encodings.get('attention_mask'))\n",
        "    \n",
        "    # return (src_ids, src_attention_masks), (tgt_ids, tgt_attention_masks)\n",
        "    return src_ids, tgt_ids"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECYFyKk48DVi"
      },
      "source": [
        "def create_token_type_ids(src, tgt):\n",
        "  src_token_type_ids = torch.zeros(src.shape, dtype=torch.int64, device=DEVICE)\n",
        "  tgt_token_type_ids = torch.zeros(tgt.shape, dtype=torch.int64, device=DEVICE)\n",
        "\n",
        "  return src_token_type_ids, tgt_token_type_ids"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXWcg1kJ37D7"
      },
      "source": [
        "def read_best_result(path):\n",
        "  with open(path, 'r') as file:\n",
        "    results = file.read().splitlines()\n",
        "    results = [list(map(float, result.split())) for result in results]\n",
        "\n",
        "  return results[-1]\n",
        "\n",
        "def write_best_result(path, train_loss, valid_loss, is_best=False):\n",
        "  with open(path, 'r+') as file:\n",
        "    results = file.read().splitlines()\n",
        "    results = [list(map(float, result.split())) for result in results]\n",
        "    results.insert(-1, [train_loss, valid_loss])\n",
        "    if is_best:\n",
        "      results.pop(-1)\n",
        "      results.append([train_loss, valid_loss])\n",
        "    file.seek(0)\n",
        "    file.truncate(0)\n",
        "    results = [' '.join(map(str, result)) for result in results]\n",
        "    for result in results:\n",
        "      file.write(result + '\\n')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sPmaP96RsHR"
      },
      "source": [
        "def convert_ids_to_string(tokenizer, ids):\n",
        "  \"\"\"\n",
        "    convert list ids (not tensor) to string\n",
        "  \"\"\"\n",
        "  tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "  sentence = \" \".join(tokens).replace(\"@@ \", \"\").replace(\"<unk> \", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
        "  return sentence"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkbUVFWlW6zC"
      },
      "source": [
        "def segment_file(src_path, tgt_path):\n",
        "  with open(src_path, 'r') as src_file:\n",
        "    with open(tgt_path, 'w') as tgt_file:\n",
        "      src = src_file.read().splitlines()\n",
        "      segmented = []\n",
        "      for sentence in tqdm(src):\n",
        "        temp = segment_sentence(sentence)\n",
        "        segmented.append(temp)\n",
        "      tgt = \"\\n\".join(segmented)\n",
        "      tgt_file.write(tgt)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(src_sentence, tgt_sentence):\n",
        "    src_encodings = src_tokenizer.batch_encode_plus([src_sentence])\n",
        "    src_ids = torch.tensor(src_encodings.get('input_ids'))\n",
        "    # src_attention_masks = torch.tensor(src_encodings.get('attention_mask'))\n",
        "    \n",
        "    tgt_encodings = tgt_tokenizer.batch_encode_plus([tgt_sentence])\n",
        "    tgt_ids = torch.tensor(tgt_encodings.get('input_ids'))\n",
        "    # tgt_attention_masks = torch.tensor(tgt_encodings.get('attention_mask'))\n",
        "    \n",
        "    return src_ids, tgt_ids"
      ],
      "metadata": {
        "id": "Iqj2af1fJUXe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_transformer(model, src_sentence, tgt_sentence, return_tgt=False):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    src, tgt = extract_features(src_sentence, tgt_sentence)\n",
        "    src = src.to(DEVICE)\n",
        "    tgt = tgt.to(DEVICE)\n",
        "    tgt_input = tgt[:, :-1]\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "    logits = model(src, \n",
        "                    tgt_input, \n",
        "                    src_mask=src_mask, \n",
        "                    tgt_mask=tgt_mask, \n",
        "                    src_key_padding_mask=src_padding_mask, \n",
        "                    tgt_key_padding_mask=tgt_padding_mask, \n",
        "                    memory_key_padding_mask=src_padding_mask)\n",
        "    if return_tgt:\n",
        "      return logits, tgt\n",
        "    return logits"
      ],
      "metadata": {
        "id": "dZYMsK_0I8kU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SINsPiHOcjF"
      },
      "source": [
        "### 6. Train and Valid Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU9fVcIU2kE9"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = CustomDataset(train_src_path, train_tgt_path)\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=2, shuffle=True)        \n",
        "    cnt = 0\n",
        "    for src, tgt in tqdm(train_dataloader, desc='Training'):\n",
        "        cnt += 1\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "        logits = model(src, \n",
        "                       tgt_input, \n",
        "                       src_mask=src_mask, \n",
        "                       tgt_mask=tgt_mask, \n",
        "                       src_key_padding_mask=src_padding_mask, \n",
        "                       tgt_key_padding_mask=tgt_padding_mask, \n",
        "                       memory_key_padding_mask=src_padding_mask)\n",
        "        logits = logits.output\n",
        "\n",
        "        optimizer.optimizer.zero_grad()\n",
        "        # optimizer.zero_grad()\n",
        "        tgt_out = tgt[:, 1:]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "        if cnt % 1000 == 0:\n",
        "            torch.save(model.state_dict(), FOLDER_PATH + \"middle.pt\")\n",
        "            torch.save(optimizer.state_dict(), FOLDER_PATH + \"optimizer.pt\")\n",
        "    \n",
        "    torch.save(model.state_dict(), FOLDER_PATH + \"middle.pt\")\n",
        "    torch.save(optimizer.state_dict(), FOLDER_PATH + \"optimizer.pt\")\n",
        "\n",
        "    return losses / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = CustomDataset(dev_src_path, dev_tgt_path)\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=2, shuffle=True)\n",
        "\n",
        "    for src, tgt in tqdm(val_dataloader, desc='Testing '):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "        logits = model(src, \n",
        "                       tgt_input, \n",
        "                       src_mask=src_mask, \n",
        "                       tgt_mask=tgt_mask,\n",
        "                       src_key_padding_mask=src_padding_mask, \n",
        "                       tgt_key_padding_mask=tgt_padding_mask, \n",
        "                       memory_key_padding_mask=src_padding_mask)\n",
        "        logits = logits.output\n",
        "        tgt_out = tgt[:, 1:]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(val_dataloader)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWu__0yNOrPW"
      },
      "source": [
        "### 7. Initialize Transformer by Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EluyIz3ky0j"
      },
      "source": [
        "def freeze_layer(layer):\n",
        "  for p in layer.parameters():\n",
        "    p.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o614q9qmh65l"
      },
      "source": [
        "def copy_encoder_layer_parameters(encoder_layer, bert_layer, freeze=False):\n",
        "  encoder_layer.self_attention.query_projection.weight.data.copy_(bert_layer.attention.self.query.weight.data)\n",
        "  encoder_layer.self_attention.key_projection.weight.data.copy_(bert_layer.attention.self.key.weight.data)\n",
        "  encoder_layer.self_attention.value_projection.weight.data.copy_(bert_layer.attention.self.value.weight.data)\n",
        "  # encoder_layer.self_attention.dropout.weight.data.copy_(bert_layer.attention.self.dropout.weight.data)\n",
        "  \n",
        "  encoder_layer.self_attention.weight_matrix.weight.data.copy_(bert_layer.attention.output.dense.weight.data)\n",
        "  # encoder_layer.dropout1.weight.data.copy_(bert_layer.attention.output.dropout.weight.data)\n",
        "  # encoder_layer.norm1.weight.data.copy_(bert_layer.attention.output.LayerNorm.weight.data)\n",
        "\n",
        "  encoder_layer.linear1.weight.data.copy_(bert_layer.intermediate.dense.weight.data)\n",
        "  encoder_layer.linear2.weight.data.copy_(bert_layer.output.dense.weight.data)\n",
        "\n",
        "  # encoder_layer.dropout2.weight.data.copy_(bert_layer.output.dropout.weight.data)\n",
        "  # encoder_layer.norm2.weight.data.copy_(bert_layer.output.LayerNorm.weight.data)\n",
        "\n",
        "  if freeze:\n",
        "    freeze_layer(encoder_layer.self_attention.query_projection)\n",
        "    freeze_layer(encoder_layer.self_attention.key_projection)\n",
        "    freeze_layer(encoder_layer.self_attention.value_projection)\n",
        "    freeze_layer(encoder_layer.self_attention.weight_matrix)\n",
        "\n",
        "    freeze_layer(encoder_layer.linear1)\n",
        "    freeze_layer(encoder_layer.linear2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isDYA_HgmzQU"
      },
      "source": [
        "def copy_decoder_layer_parameters(decoder_layer, bert_layer, freeze=False):\n",
        "  decoder_layer.self_attention.query_projection.weight.data.copy_(bert_layer.attention.self.query.weight.data)\n",
        "  decoder_layer.self_attention.key_projection.weight.data.copy_(bert_layer.attention.self.key.weight.data)\n",
        "  decoder_layer.self_attention.value_projection.weight.data.copy_(bert_layer.attention.self.value.weight.data)\n",
        "  # decoder_layer.self_attention.dropout.weight.data.copy_(bert_layer.attention.self.dropout.weight.data)\n",
        "  \n",
        "  decoder_layer.self_attention.weight_matrix.weight.data.copy_(bert_layer.attention.output.dense.weight.data)\n",
        "  # decoder_layer.dropout1.weight.data.copy_(bert_layer.attention.output.dropout.weight.data)\n",
        "  # decoder_layer.norm1.weight.data.copy_(bert_layer.attention.output.LayerNorm.weight.data)\n",
        "\n",
        "  decoder_layer.linear1.weight.data.copy_(bert_layer.intermediate.dense.weight.data)\n",
        "  decoder_layer.linear2.weight.data.copy_(bert_layer.output.dense.weight.data)\n",
        "\n",
        "  # decoder_layer.dropout2.weight.data.copy_(bert_layer.output.dropout.weight.data)\n",
        "  # decoder_layer.norm2.weight.data.copy_(bert_layer.output.LayerNorm.weight.data)\n",
        "\n",
        "  if freeze:\n",
        "    freeze_layer(decoder_layer.self_attention.query_projection)\n",
        "    freeze_layer(decoder_layer.self_attention.key_projection)\n",
        "    freeze_layer(decoder_layer.self_attention.value_projection)\n",
        "    freeze_layer(decoder_layer.self_attention.weight_matrix)\n",
        "\n",
        "    freeze_layer(decoder_layer.linear1)\n",
        "    freeze_layer(decoder_layer.linear2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-qsRwY0peWL"
      },
      "source": [
        "def copy_src_embeddings_parameters_from_bert(transformer, src_bert, freeze=False):\n",
        "  transformer.src_embedding.word_embedding.weight.data.copy_(src_bert.embeddings.word_embeddings.weight.data)\n",
        "  transformer.src_embedding.position_embedding.weight.data[:512, :].copy_(src_bert.embeddings.position_embeddings.weight.data)\n",
        "  transformer.src_embedding.token_type_embedding.weight.data.copy_(src_bert.embeddings.token_type_embeddings.weight.data[:1, :])\n",
        "  # transformer.src_embedding.norm.weight.data.copy_(src_bert.embeddings.LayerNorm.weight.data)\n",
        "  # transformer.src_embedding.dropout.weight.data.copy_(src_bert.embeddings.dropout.weight.data)\n",
        "\n",
        "  if freeze:\n",
        "    freeze_layer(transformer.src_embedding.word_embedding)\n",
        "    freeze_layer(transformer.src_embedding.position_embedding)\n",
        "    freeze_layer(transformer.src_embedding.token_type_embedding)\n",
        "    # freeze_layer(transformer.src_embedding.norm)\n",
        "    # freeze_layer(transformer.src_embedding.dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_tgt_embeddings_parameters_from_bert(transformer, tgt_bert, freeze=False):\n",
        "  transformer.tgt_embedding.word_embedding.weight.data.copy_(tgt_bert.embeddings.word_embeddings.weight.data)\n",
        "  transformer.tgt_embedding.position_embedding.weight.data[:258, :].copy_(tgt_bert.embeddings.position_embeddings.weight.data)\n",
        "  transformer.tgt_embedding.token_type_embedding.weight.data.copy_(tgt_bert.embeddings.token_type_embeddings.weight.data)\n",
        "  # transformer.tgt_embedding.norm.weight.data.copy_(tgt_bert.embeddings.LayerNorm.weight.data)\n",
        "  # transformer.tgt_embedding.dropout.weight.data.copy_(tgt_bert.embeddings.dropout.weight.data)\n",
        "  \n",
        "\n",
        "  if freeze:\n",
        "    freeze_layer(transformer.tgt_embedding.word_embedding)\n",
        "    freeze_layer(transformer.tgt_embedding.position_embedding)\n",
        "    freeze_layer(transformer.tgt_embedding.token_type_embedding)\n",
        "    # freeze_layer(transformer.tgt_embedding.norm)\n",
        "    # freeze_layer(transformer.tgt_embedding.dropout)"
      ],
      "metadata": {
        "id": "DYvVKQX5Ab5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98w7u84fN3y-"
      },
      "source": [
        "def copy_encoder_parameters_from_bert(encoder, bert, freeze=False):\n",
        "  number_encoder_layers = len(encoder)\n",
        "  number_bert_layers = len(bert)\n",
        "  # if number_encoder_layers != number_bert_layers:\n",
        "  #   raise RuntimeError(f\"number of encoder layers of two models must be equal, but got {number_encoder_layers} and {number_bert_layers}\")\n",
        "  for index in range(number_encoder_layers):\n",
        "    encoder_layer = encoder[index]\n",
        "    bert_layer = bert[index]\n",
        "    copy_encoder_layer_parameters(encoder_layer, bert_layer, freeze)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aGne685RMCx"
      },
      "source": [
        "def copy_decoder_parameters_from_bert(decoder, bert, freeze=False):\n",
        "  number_decoder_layers = len(decoder)\n",
        "  number_bert_layers = len(bert)\n",
        "  # if number_decoder_layers != number_bert_layers:\n",
        "  #   raise RuntimeError(f\"number of decoder layers of two models must be equal, but got {number_decoder_layers} and {number_bert_layers}\")\n",
        "  for index in range(number_decoder_layers):\n",
        "    decoder_layer = decoder[index]\n",
        "    bert_layer = bert[index]\n",
        "    copy_decoder_layer_parameters(decoder_layer, bert_layer, freeze)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWD4KupLzi0f"
      },
      "source": [
        "def copy_transformer_parameters_from_berts(transformer, src_bert, tgt_bert, freeze=False):\n",
        "  copy_encoder_parameters_from_bert(transformer.transformer.encoder.layers, src_bert.encoder.layer, freeze)\n",
        "  copy_decoder_parameters_from_bert(transformer.transformer.decoder.layers, tgt_bert.encoder.layer, freeze)\n",
        "  copy_src_embeddings_parameters_from_bert(transformer, src_bert, freeze)\n",
        "  copy_tgt_embeddings_parameters_from_bert(transformer, tgt_bert, freeze)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4azMpjTsuUfa"
      },
      "source": [
        "# # using for BERT Encoder\n",
        "# copy_encoder_parameters_from_bert(transformer.transformer.encoder.layers, src_bert.encoder.layer, freeze)\n",
        "# copy_src_embeddings_parameters_from_bert(transformer, src_bert, freeze)\n",
        "\n",
        "# using for BERT Encoder-Decoder\n",
        "copy_transformer_parameters_from_berts(transformer, src_bert, tgt_bert, freeze=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWoP3tIXOy90"
      },
      "source": [
        "### 8. Search Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuiWJ0Etl3Ne"
      },
      "source": [
        "# function to generate output sequence using greedy algorithm \n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    # print(src)\n",
        "    # print(src_mask)\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        # print(ys.shape, tgt_mask.shape)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # print(prob)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        if next_word == TGT_EOS_ID:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src_encodings = src_tokenizer.batch_encode_plus([src_sentence], padding=True)\n",
        "    src_ids = torch.tensor(src_encodings.get('input_ids'))\n",
        "    num_tokens = src_ids.shape[1]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src_ids, src_mask, max_len=num_tokens*2+5, start_symbol=TGT_BOS_ID).flatten()\n",
        "    return convert_ids_to_string(tgt_tokenizer, tgt_tokens.tolist())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfnzziyl3xV"
      },
      "source": [
        "def beam_search(model, src, src_mask, max_len, start_symbol, num_beams, k, is_train=False):\n",
        "  max_len = 256 if max_len > 256 else max_len\n",
        "  src = src.to(DEVICE)\n",
        "  src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "  memory = model.encode(src, src_mask)\n",
        "  ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "  beam_results = [[ys, 0.0]]\n",
        "  for i in range(max_len-1):\n",
        "    beam_candidates = []\n",
        "    is_change = False\n",
        "    for beam in beam_results:\n",
        "      ys = beam[0]\n",
        "      if ys[-1][0] == TGT_EOS_ID:\n",
        "        beam_candidates.append(beam)\n",
        "        continue\n",
        "      else:\n",
        "        is_change = True\n",
        "      tgt_mask = (generate_square_subsequent_mask(ys.size(1)).type(torch.bool)).to(DEVICE)\n",
        "      out = model.decode(ys, memory, tgt_mask)\n",
        "      prob = model.generator(out[:, -1])\n",
        "      prob = F.log_softmax(prob, dim=-1)\n",
        "      topk = torch.topk(prob, k, dim=1)\n",
        "      indices = topk.indices[0]\n",
        "      values = topk.values[0]\n",
        "      for index in range(num_beams):\n",
        "        ids = torch.cat([beam[0], torch.ones(1, 1).type_as(src.data).fill_(indices[index])], dim=1)\n",
        "        score = beam[1] - values[index]\n",
        "        beam_candidates.append([ids, score])\n",
        "\n",
        "    beam_candidates = sorted(beam_candidates, key=lambda x: x[1])\n",
        "    beam_results = beam_candidates[:num_beams]\n",
        "    if not is_change:\n",
        "      break\n",
        "  \n",
        "  return beam_results[0][0]\n",
        "\n",
        "def beam_translate(model: torch.nn.Module, src_sentence: str, num_beams: int = 4, k: int = 4):\n",
        "    model.eval()\n",
        "    src_encodings = src_tokenizer.batch_encode_plus([src_sentence], padding=True)\n",
        "    src_ids = torch.tensor(src_encodings.get('input_ids'))\n",
        "    num_tokens = src_ids.shape[1]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = beam_search(\n",
        "        model,  src_ids, src_mask, max_len=num_tokens*2+5, start_symbol=TGT_BOS_ID, num_beams=num_beams, k=k).flatten()\n",
        "    return convert_ids_to_string(tgt_tokenizer, tgt_tokens.tolist())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL0oPyWRaJaN"
      },
      "source": [
        "def init_beam_search(model, src, src_mask, max_len, start_symbol, num_beams):\n",
        "  \"\"\"\n",
        "  init first beam search with BOS symbol and calculate topk candidates\n",
        "  :param model:\n",
        "  :param src: (N, S), batch size N = 1\n",
        "  :param src_mask: (S, S)\n",
        "  :param max_len: \n",
        "  :param start_symbol: BOS\n",
        "  :param num_beams:\n",
        "  :return: outputs (num_beams, max_len), memory (N,S,E), log_scores (N, num_beams) \n",
        "  \"\"\"\n",
        "  src = src.to(DEVICE)\n",
        "  src_mask = src_mask.to(DEVICE)\n",
        "  memory = model.encode(src, src_mask)\n",
        "  batch_size, src_length, hidden_size = memory.shape\n",
        "  tgt = torch.LongTensor([[start_symbol]]).to(DEVICE)\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt.size(1)).type(torch.bool)\n",
        "  outputs = model.decode(tgt, memory, tgt_mask)\n",
        "  outputs = model.generator(outputs[:, -1])\n",
        "  log_scores, index = F.log_softmax(outputs, dim=-1).topk(num_beams)\n",
        "  outputs = torch.zeros((num_beams, max_len), dtype=torch.int32, device=DEVICE)\n",
        "  outputs[:, 0] = start_symbol\n",
        "  outputs[:, 1] = index[0]\n",
        "  memory = memory.expand(num_beams, src_length, hidden_size)\n",
        "\n",
        "  return outputs, memory, log_scores"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcbPbR1suJOB"
      },
      "source": [
        "def choose_topk(outputs, prob, log_scores, i, num_beams):\n",
        "  \"\"\"\n",
        "  choose topk candidates from kxk candidates\n",
        "  \"\"\"\n",
        "  log_probs, index = F.log_softmax(prob, dim=-1).topk(num_beams)\n",
        "  # log_scores.transpose(0,1) to add correct element to log_probs\n",
        "  log_probs = log_probs + log_scores.transpose(0, 1)\n",
        "  log_probs, k_index = log_probs.view(-1).topk(num_beams)\n",
        "\n",
        "  # calculate rows, cols becasue log_probs now has shape (num_beams x num_beams)\n",
        "  rows = torch.div(k_index, num_beams, rounding_mode='floor')\n",
        "  cols = k_index % num_beams\n",
        "  outputs[:, :i] = outputs[rows, :i]\n",
        "  outputs[:, i] = index[rows, cols]\n",
        "  \n",
        "  # log_probs has shape (num_beams) -> (1, num_beams)\n",
        "  log_scores = log_probs.unsqueeze(0)\n",
        "\n",
        "  return outputs, log_scores"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMNUixpOBiE2"
      },
      "source": [
        "def model_generate(model,\n",
        "                   tgt: Tensor,\n",
        "                   memory: Tensor,\n",
        "                   tgt_mask: Optional[Tensor] = None):\n",
        "  \n",
        "  prob = model.decode(tgt, memory, tgt_mask)\n",
        "  prob = model.generator(prob[:, -1])\n",
        "  return prob"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hzRp6qLEZW2"
      },
      "source": [
        "def beam_generate(model, outputs, memory, log_scores, i, num_beams):\n",
        "  tgt_mask = generate_square_subsequent_mask(outputs[:, :i].size(1)).type(torch.bool)\n",
        "  prob = model_generate(model, outputs[:, :i], memory, tgt_mask)\n",
        "  return choose_topk(outputs, prob, log_scores, i, num_beams)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRFvv_lpUcI3"
      },
      "source": [
        "def beam_search_1(model,src, src_mask, max_len, start_symbol, end_symbol, num_beams):\n",
        "  max_len = 256 if max_len > 256 else max_len\n",
        "  chosen_sentence_index = 0\n",
        "  outputs, memory, log_scores = init_beam_search(model, src, src_mask, max_len, start_symbol, num_beams)\n",
        "  for i in range(2, max_len):\n",
        "    tgt_mask = generate_square_subsequent_mask(outputs[:, :i].size(1)).type(torch.bool)\n",
        "    # prob = model_generate(model, outputs[:, :i], memory, tgt_mask)\n",
        "    prob = model.decode(outputs[:, :i], memory, tgt_mask[:i, :i])\n",
        "    prob = model.generator(prob[:, -1])\n",
        "    outputs, log_scores = choose_topk(outputs, prob, log_scores, i, num_beams)\n",
        "    # outputs, log_scores = beam_generate(model, outputs, memory, log_scores, i, num_beams)\n",
        "    finished_sentences = (outputs == end_symbol).nonzero()\n",
        "    mark_eos = torch.zeros(num_beams, dtype=torch.int64, device=DEVICE)\n",
        "    num_finished_sentences = 0\n",
        "    for eos_symbol in finished_sentences:\n",
        "      sentence_ind, eos_location = eos_symbol\n",
        "      if mark_eos[sentence_ind] == 0:\n",
        "        mark_eos[sentence_ind] = eos_location\n",
        "        num_finished_sentences += 1\n",
        "    \n",
        "    if num_finished_sentences == num_beams:\n",
        "      alpha = 0.7\n",
        "      division = mark_eos.type_as(log_scores)**alpha\n",
        "      _, chosen_sentence_index = torch.max(log_scores / division, 1)\n",
        "      chosen_sentence_index = chosen_sentence_index[0]\n",
        "      break\n",
        "  \n",
        "  sentence_length = (outputs[chosen_sentence_index] == end_symbol).nonzero()\n",
        "  sentence_length = sentence_length[0] if len(sentence_length) > 0 else -1\n",
        "  return outputs[chosen_sentence_index][:sentence_length+1]\n",
        "\n",
        "def beam_translate_1(model: torch.nn.Module, src_sentence: str, num_beams: int = 3):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      src_encodings = src_tokenizer.batch_encode_plus([src_sentence], padding=True)\n",
        "      src_ids = torch.tensor(src_encodings.get('input_ids'))\n",
        "      num_tokens = src_ids.shape[1]\n",
        "      src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "      tgt_tokens = beam_search_1(\n",
        "          model,  src_ids, src_mask, max_len=int(num_tokens*1.5+5), start_symbol=TGT_BOS_ID, end_symbol=TGT_EOS_ID ,num_beams=num_beams).flatten()\n",
        "      return convert_ids_to_string(tgt_tokenizer, tgt_tokens.tolist())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OindoLVZp-XU"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def bleu(src_path, tgt_path, model, beam_search=True, num_beams=3, k=3, return_sentence=False):\n",
        "    model.eval()\n",
        "    data_iter = CustomDataset(src_path, tgt_path)\n",
        "    pred_sents = []\n",
        "    tgt_sents = []\n",
        "    # search_func = beam_translate_1 if beam_search else translate\n",
        "    for src, tgt in tqdm(data_iter, desc='Blue score'):\n",
        "      if beam_search:\n",
        "        pred_tgt = beam_translate_1(model, src, num_beams=num_beams)\n",
        "      else:\n",
        "        pred_tgt = translate(model, src)\n",
        "      pred_sents.append(pred_tgt)\n",
        "      tgt_sents.append(tgt)\n",
        "\n",
        "    translation_sents = [sent.strip().replace('_', ' ').split() for sent in pred_sents]\n",
        "    target_sents = [[sent.strip().replace('_', ' ').split()] for sent in tgt_sents]\n",
        "    \n",
        "    bleu = bleu_score(translation_sents, target_sents)\n",
        "    if return_sentence:\n",
        "      return bleu, pred_sents, tgt_sents\n",
        "    else:\n",
        "      return bleu"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caviQnKdO9mM"
      },
      "source": [
        "### 9. Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 2\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    best_train_lost, best_val_loss = read_best_result(RESULT_PATH)\n",
        "    is_best = True if val_loss < best_val_loss else False\n",
        "    write_best_result(RESULT_PATH, train_loss, val_loss, is_best)\n",
        "    if is_best:\n",
        "      torch.save(transformer.state_dict(), FOLDER_PATH + \"best.pt\")"
      ],
      "metadata": {
        "id": "PASE0OhNxLnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiX8U3KCYzHM"
      },
      "source": [
        "bleu(test_src_path, test_tgt_path, transformer, num_beams=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vurUGHjhNEd"
      },
      "source": [
        "# trained model path\n",
        "folder = \"https://drive.google.com/drive/folders/1HkRLj9iTdUi1pPUk_hU0fXH2BERAsCXf?usp=sharing\"\n",
        "best_bert = 'https://drive.google.com/file/d/1a5-iSc08WdpZmIWmQezBTKSpWI3RoU17/view?usp=sharing'\n",
        "long_dataset_70000_index = 'https://drive.google.com/file/d/1H0WgrRJxmYuZcw3qoYEd_tGv22lUkvWx/view?usp=sharing'\n",
        "medical_dataset_70000_index = 'https://drive.google.com/file/d/1FlKCWtemEUfWDEggMD5_2guxtEXEOBVh/view?usp=sharing'\n",
        "medical_vals = 'https://drive.google.com/file/d/1cciP8LLqUlYddYuGPbxZdOD-VGmsTQdn/view?usp=sharing'\n",
        "vals = 'https://drive.google.com/file/d/1fBBtd7eYbk8VGk-cy5pMH32oXrqPJQE1/view?usp=sharing'\n",
        "\n",
        "# please download and move to a folder, enter folder path here\n",
        "PATH = './'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load best model\n",
        "transformer.load_state_dict(torch.load(PATH + 'best-NMT.pt'))"
      ],
      "metadata": {
        "id": "Xsvy-MapOWeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erq6bHt0LTLH"
      },
      "source": [
        "with open(test_src_path, 'r') as file:\n",
        "  contents = file.read().splitlines()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y18gKs3kO7wH"
      },
      "source": [
        "translated_sents = [beam_translate_1(transformer, sentence) for sentence in contents[:10]] "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll8d3e9VMoxo"
      },
      "source": [
        "translated_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz6bmLnRRBaW"
      },
      "source": [
        "import torch\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kNN-MT"
      ],
      "metadata": {
        "id": "tNHGNGEgIFs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### for cpu\n",
        "# !apt install libomp-dev\n",
        "# !pip install faiss\n",
        "# ### for gpu\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "o-3nAM8mIM9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "UOMvTzSVM84Q"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Transformer.application.NMT import Datastore, DatastoreBuilder, NMTModel, TranslateMachine, CustomDataset, calculate_bleu_score "
      ],
      "metadata": {
        "id": "zpy5okg8OFpP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmt_model = NMTModel(SRC_BOS_ID, SRC_EOS_ID, TGT_BOS_ID, TGT_EOS_ID, src_tokenizer, tgt_tokenizer, config, transformer)"
      ],
      "metadata": {
        "id": "Go2VjWsNPcXf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build datastore and kNN-MT\n",
        "\n",
        "datastore_builder = DatastoreBuilder(nmt_model, DEVICE)\n",
        "embeddings_results, vals = datastore_builder.batch_create_features_file(medical_train_src_path, medical_train_tgt_path, batch_size=20, end_index=70000)\n",
        "np.save(FOLDER_PATH + 'medical_vals', vals)\n",
        "\n",
        "data_store_length = get_data_store_length(medical_train_src_path, medical_train_tgt_path, end_index=70000)\n",
        "data_store = Datastore(768, size_value_array=TGT_VOCAB_SIZE, num_centroid=128, nprobe=32)\n",
        "data_store.build_datastore(embeddings_results)\n",
        "data_store.save_index(FOLDER_PATH + 'medical_dataset_70000_index')\n",
        "\n",
        "translate_machine = TranslateMachine(nmt_model, data_store, vals, device=DEVICE)\n",
        "data_store_length"
      ],
      "metadata": {
        "id": "N5wZ-I_mGHrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load built datastore and kNN-MT for normal dataset\n",
        "\n",
        "import numpy as np\n",
        "load_path = PATH + 'long_dataset_70000_index'\n",
        "val_path = PATH + 'vals.npy'\n",
        "\n",
        "data_store = Datastore(768, size_value_array=TGT_VOCAB_SIZE, num_centroid=128, nprobe=32, load_file=load_path)\n",
        "# data_store.build_datastore(embeddings_results)\n",
        "vals = np.load(val_path)\n",
        "translate_machine = TranslateMachine(nmt_model, data_store, vals, use_layernorm=False, device=DEVICE)"
      ],
      "metadata": {
        "id": "GZ6vpGwAOBx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load built datastore and kNN-MT for medical dataset\n",
        "\n",
        "import numpy as np\n",
        "load_path = PATH + 'medical_dataset_70000_index'\n",
        "val_path = PATH + 'medical_vals.npy'\n",
        "\n",
        "data_store = Datastore(768, size_value_array=TGT_VOCAB_SIZE, num_centroid=128, nprobe=32, load_file=load_path)\n",
        "# data_store.build_datastore(embeddings_results)\n",
        "vals = np.load(val_path)\n",
        "translate_machine = TranslateMachine(nmt_model, data_store, vals, device=DEVICE)"
      ],
      "metadata": {
        "id": "XQSfNltBM2Yk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"The distribution of the hemorrhage suggests a possible aneurysm of the left middle cerebral artery as a source.\""
      ],
      "metadata": {
        "id": "gkw4hrjYUlXE"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_machine.beam_translate(txt, num_knns=64)"
      ],
      "metadata": {
        "id": "u3AdXsaIUo-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_bleu_score(translate_machine, medical_test_src_path, medical_test_tgt_path, num_knns=64, gamma=0.4)"
      ],
      "metadata": {
        "id": "Erh3yg1JUuCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}