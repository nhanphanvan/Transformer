{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhanphanvan/Transformer/blob/main/NMT_Demo_Use_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI-2XTz9lqQi"
      },
      "source": [
        "### Install Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3H_l97MUFMY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXDTbUJPlpg9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nhanphanvan/Transformer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb7rOiXXY_tp"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWbx5DmTl8Vx"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers\n",
        "# !pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J6DIIB_nU3J"
      },
      "outputs": [],
      "source": [
        "# ### for cpu\n",
        "# !apt install libomp-dev\n",
        "# !pip install faiss\n",
        "# ### for gpu\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnPdY0dEnaMS"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trained model path\n",
        "folder = \"https://drive.google.com/drive/folders/1HkRLj9iTdUi1pPUk_hU0fXH2BERAsCXf?usp=sharing\"\n",
        "best_bert = 'https://drive.google.com/file/d/1a5-iSc08WdpZmIWmQezBTKSpWI3RoU17/view?usp=sharing'\n",
        "long_dataset_70000_index = 'https://drive.google.com/file/d/1H0WgrRJxmYuZcw3qoYEd_tGv22lUkvWx/view?usp=sharing'\n",
        "medical_dataset_70000_index = 'https://drive.google.com/file/d/1FlKCWtemEUfWDEggMD5_2guxtEXEOBVh/view?usp=sharing'\n",
        "medical_vals = 'https://drive.google.com/file/d/1cciP8LLqUlYddYuGPbxZdOD-VGmsTQdn/view?usp=sharing'\n",
        "vals = 'https://drive.google.com/file/d/1fBBtd7eYbk8VGk-cy5pMH32oXrqPJQE1/view?usp=sharing'\n",
        "\n",
        "# please download and move to a folder, enter folder path here\n",
        "PATH = './'"
      ],
      "metadata": {
        "id": "TaZNepTCOnGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOnRRME6mpq4"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP6lYwNAnqXR"
      },
      "source": [
        "### Machine Translation Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2V-59NaMqeG"
      },
      "outputs": [],
      "source": [
        "%%writefile setup.py\n",
        "import torch\n",
        "\n",
        "# please enter folder path here\n",
        "FOLDER_PATH = \"./\"\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SRC_VOCAB_SIZE = 28996\n",
        "TGT_VOCAB_SIZE = 64001\n",
        "HIDDEN_SIZE = 768\n",
        "NUM_ENCODER_LAYERS = 12\n",
        "NUM_DECODER_LAYERS = 12\n",
        "NUM_ATTENTION_HEADS = 12\n",
        "FEEDFORWARD_SIZE = 3072\n",
        "DROPOUT = 0.1\n",
        "ACTIVATION = 'gelu'\n",
        "LAYER_NORM_EPS = 1e-12\n",
        "SRC_UNK_ID, SRC_PADDING_ID, SRC_BOS_ID, SRC_EOS_ID = 100, 0, 101, 102\n",
        "TGT_UNK_ID, TGT_PADDING_ID, TGT_BOS_ID, TGT_EOS_ID = 3, 1, 0, 2\n",
        "NORM_FIRST = True\n",
        "MAX_SEQUENCE_LENGTH = 1024\n",
        "BATCH_SIZE = 10\n",
        "BERT_EMBEDDING = True\n",
        "OUTPUT_HIDDEN_STATES = True\n",
        "APPLY_LAYER_NORM = True\n",
        "\n",
        "############################################################################\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "src_model_id = 'bert-base-cased'\n",
        "tgt_model_id = 'vinai/phobert-base'\n",
        "\n",
        "src_config = AutoConfig.from_pretrained(src_model_id)\n",
        "# src_bert = AutoModel.from_pretrained(src_model_id, config=src_config)\n",
        "src_tokenizer = AutoTokenizer.from_pretrained(src_model_id)\n",
        "src_tokenizer.model_max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "tgt_config = AutoConfig.from_pretrained(tgt_model_id)\n",
        "# tgt_bert = AutoModel.from_pretrained(tgt_model_id, config=tgt_config)\n",
        "tgt_tokenizer = AutoTokenizer.from_pretrained(tgt_model_id)\n",
        "tgt_tokenizer.model_max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "############################################################################\n",
        "from Transformer.modules.config import TransformerConfig\n",
        "from Transformer.modules.transformer import Transformer\n",
        "from Transformer.modules.embedding import PositionalEmbedding, TransformerEmbedding\n",
        "from Transformer.modules.seq2seq_transformer import Seq2SeqTransformer\n",
        "\n",
        "kwargs = {\n",
        "    'src_vocab_size': SRC_VOCAB_SIZE,\n",
        "    'tgt_vocab_size': TGT_VOCAB_SIZE,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'num_encoder_layers': NUM_ENCODER_LAYERS,\n",
        "    'num_decoder_layers': NUM_DECODER_LAYERS,\n",
        "    'num_attention_heads': NUM_ATTENTION_HEADS,\n",
        "    'feedforward_size': FEEDFORWARD_SIZE,\n",
        "    'dropout': DROPOUT,\n",
        "    'activation': ACTIVATION,\n",
        "    'layer_norm_eps': LAYER_NORM_EPS,\n",
        "    'src_padding_id': SRC_PADDING_ID,\n",
        "    'tgt_padding_id': TGT_PADDING_ID,\n",
        "    'norm_first': NORM_FIRST,\n",
        "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
        "    'bert_embedding': BERT_EMBEDDING,\n",
        "    'output_hidden_states': OUTPUT_HIDDEN_STATES,\n",
        "    'apply_layer_norm': APPLY_LAYER_NORM,\n",
        "    'device': DEVICE,\n",
        "    'dtype': torch.float32\n",
        "}\n",
        "\n",
        "config = TransformerConfig(**kwargs)\n",
        "transformer = Seq2SeqTransformer(config=config)\n",
        "transformer = transformer.to(DEVICE)\n",
        "transformer.load_state_dict(torch.load(FOLDER_PATH + 'best-NMT.pt'))\n",
        "##########################################################################\n",
        "from Transformer.application.NMT import Datastore, DatastoreBuilder, NMTModel, TranslateMachine, CustomDataset, calculate_bleu_score\n",
        "import numpy as np\n",
        "\n",
        "load_path_1 = FOLDER_PATH + 'long_dataset_70000_index'\n",
        "val_path_1 = FOLDER_PATH + 'vals.npy'\n",
        "\n",
        "load_path_2 = FOLDER_PATH + 'medical_dataset_70000_index'\n",
        "val_path_2 = FOLDER_PATH + 'medical_vals.npy'\n",
        "\n",
        "nmt_model = NMTModel(SRC_BOS_ID, SRC_EOS_ID, TGT_BOS_ID, TGT_EOS_ID, src_tokenizer, tgt_tokenizer, config, transformer)\n",
        "# datastore_builder = DatastoreBuilder(nmt_model, DEVICE)\n",
        "# embeddings_results, vals = datastore_builder.batch_create_features_file(long_src_path, long_src_path, batch_size=20, end_index=70000)\n",
        "general_data_store = Datastore(768, size_value_array=TGT_VOCAB_SIZE, num_centroid=128, nprobe=32, load_file=load_path_1)\n",
        "medical_data_store = Datastore(768, size_value_array=TGT_VOCAB_SIZE, num_centroid=128, nprobe=32, load_file=load_path_2)\n",
        "# data_store.build_datastore(embeddings_results)\n",
        "general_vals = np.load(val_path_1)\n",
        "medical_vals = np.load(val_path_2)\n",
        "general_translate_machine = TranslateMachine(nmt_model, general_data_store, general_vals, device=DEVICE)\n",
        "medical_translate_machine = TranslateMachine(nmt_model, medical_data_store, medical_vals, device=DEVICE)\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiyEe6vbgrH0",
        "outputId": "bd35f1d1-3d12-4476-a49a-722bce62b796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from setup import general_translate_machine, medical_translate_machine\n",
        "import streamlit as st\n",
        "\n",
        "st.sidebar.subheader('Select your domain below.')\n",
        "domain = st.sidebar.selectbox(\"Domain\",['General', 'Medical'])\n",
        "\n",
        "\n",
        "st.title('Simple English ➡️ Vietnamese Translation App')\n",
        "st.write('This is a simple machine translation app that will translate\\\n",
        "         your English input text into Vietnamese language\\\n",
        "         by leveraging a pre-trained [Text-To-Text Transfer Tranformers](https://arxiv.org/abs/1910.10683) model.')\n",
        "\n",
        "st.subheader('Input Text')\n",
        "text = st.text_area(' ', height=200)\n",
        "\n",
        "if text != '':\n",
        "    \n",
        "    translate_machine = medical_translate_machine if domain == 'Medical' else general_translate_machine\n",
        "    translated_sentence_pure = translate_machine.beam_translate(text, num_knns=64, use_datastore=False)\n",
        "    translated_sentence_pure = translated_sentence_pure.strip().replace('_', ' ')\n",
        "    translated_sentence = translate_machine.beam_translate(text, num_knns=64)\n",
        "    translated_sentence = translated_sentence.strip().replace('_', ' ')\n",
        "    \n",
        "    st.subheader('Translated Text (Use Datastore)')\n",
        "    st.write(translated_sentence)\n",
        "    st.subheader('Translated Text (Not Use Datastore)')\n",
        "    st.write(translated_sentence_pure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcUTx_uharFO"
      },
      "outputs": [],
      "source": [
        "# use this if you are using colab\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6XLrnODUvgv"
      },
      "outputs": [],
      "source": [
        "# # use if you in local machine\n",
        "# !stremlit run app.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NMT-Demo-Use-Streamlit.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwcjSTZC+QjK2AXovVC8ZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}